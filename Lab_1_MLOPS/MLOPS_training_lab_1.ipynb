{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwTRErfBSV87",
        "outputId": "e125a16c-4d10-45a9-cff4-bad717b85e8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.10/dist-packages (0.1.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.2)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.16)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets\n",
        "!pip install pandas\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cv5-aXERSXZ2",
        "outputId": "4a38b973-c835-4cb7-ca90-68ce62ffcfda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping, found downloaded files in \"./fake-or-real-news\" (use force=True to force download)\n"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "import pandas\n",
        "\n",
        "od.download(\"https://www.kaggle.com/datasets/jillanisofttech/fake-or-real-news\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBcr-YkWHZ5d",
        "outputId": "07107683-75e6-4518-e6ee-7222a3e82622"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnMh9WBpHcGj",
        "outputId": "089d4b8c-6394-4913-92d0-181bd26bdddf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub('\\[[^]]*\\]', '', text)  # remove text in brackets\n",
        "    text = re.sub(r'http\\S+', '', text)  # remove URLs\n",
        "    stop_words = set(stopwords.words('english'))  # Use NLTK's stopwords\n",
        "    punctuation = list(string.punctuation)\n",
        "    stop_words.update(punctuation)  # Add punctuation to stopwords list\n",
        "    text = \" \".join(word for word in text.split() if word.lower() not in stop_words)\n",
        "    return text\n",
        "\n",
        "\n",
        "data_path = '/content/fake-or-real-news/fake_or_real_news.csv'  # Update this path with your CSV file\n",
        "data = pd.read_csv(data_path)\n",
        "\n",
        "\n",
        "data['text'] = data['text'].apply(clean_text)\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(data['text'])\n",
        "sequences = tokenizer.texts_to_sequences(data['text'])\n",
        "X = pad_sequences(sequences, maxlen=500)\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(data['label'])\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "vocab_size = 5000\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "output_dim = 1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=500))\n",
        "model.add(LSTM(hidden_dim, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
        "model.add(LSTM(hidden_dim, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(output_dim, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8zs-B8o9FRP",
        "outputId": "d4b2956b-e0ff-4410-8110-14c33b4440bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "159/159 [==============================] - ETA: 0s - loss: 0.3916 - accuracy: 0.8275\n",
            "Epoch 1: val_loss improved from inf to 0.54284, saving model to /content/drive/MyDrive/lab1_mlops_weights/model.h5\n",
            "159/159 [==============================] - 578s 4s/step - loss: 0.3916 - accuracy: 0.8275 - val_loss: 0.5428 - val_accuracy: 0.7348\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "159/159 [==============================] - ETA: 0s - loss: 0.2597 - accuracy: 0.8970\n",
            "Epoch 2: val_loss improved from 0.54284 to 0.29211, saving model to /content/drive/MyDrive/lab1_mlops_weights/model.h5\n",
            "159/159 [==============================] - 567s 4s/step - loss: 0.2597 - accuracy: 0.8970 - val_loss: 0.2921 - val_accuracy: 0.8856\n",
            "Epoch 3/3\n",
            "159/159 [==============================] - ETA: 0s - loss: 0.1136 - accuracy: 0.9597\n",
            "Epoch 3: val_loss did not improve from 0.29211\n",
            "159/159 [==============================] - 567s 4s/step - loss: 0.1136 - accuracy: 0.9597 - val_loss: 0.3141 - val_accuracy: 0.8958\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f4c200ff220>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "model_checkpoint_path = '/content/drive/MyDrive/lab1_mlops_weights/model.h5'\n",
        "checkpoint = ModelCheckpoint(model_checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=32, validation_data=(X_val, y_val), callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ffJbp58-9EIy",
        "outputId": "642a2949-42ec-472f-88fb-205f729d8077"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - 12s 288ms/step - loss: 0.3141 - accuracy: 0.8958\n",
            "Validation Loss: 0.314130961894989\n",
            "Validation Accuracy: 0.8958168625831604\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(X_val, y_val)\n",
        "print(\"Validation Loss:\", loss)\n",
        "print(\"Validation Accuracy:\", accuracy)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}